{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "We now know what shape we want our data to be in:\n",
    "- player ID (as in ```player_idlist.csv```)\n",
    "- player name (full name to avoid ambiguities)\n",
    "- position (GK/DEF/MID/FWD)\n",
    "- gameweek \n",
    "- actual FPL points scored during gameweek\n",
    "- player value (price)\n",
    "- minutes played\n",
    "- expected goals\n",
    "- expected assists\n",
    "- expected goals conceded\n",
    "- goals scored\n",
    "- assists\n",
    "- goals conceded\n",
    "- clean sheets\n",
    "- ict index\n",
    "- fixture difficulty\n",
    "\n",
    "**We want one big csv file PER SEASON.**\n",
    "\n",
    "...let's get to work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2022-23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in raw data\n",
    "merged_gws_22_23 = pd.read_csv('../data/raw/2022-23/gws/merged_gw.csv')\n",
    "\n",
    "# omit all data prior to gameweek 16 (as there was no expected goal data before that)\n",
    "merged_gws_22_23 = merged_gws_22_23[merged_gws_22_23['GW'] > 15]\n",
    "\n",
    "# pick out useful subset of data\n",
    "merged_gws_22_23 = merged_gws_22_23[\n",
    "    [\n",
    "        'element',      # player ID\n",
    "        'name',\n",
    "        'position',\n",
    "        'GW',\n",
    "        'total_points',\n",
    "        'value',\n",
    "        'minutes',\n",
    "        'expected_goals',\n",
    "        'expected_assists',\n",
    "        'expected_goals_conceded',\n",
    "        'goals_scored',\n",
    "        'assists',\n",
    "        'goals_conceded',\n",
    "        'clean_sheets',\n",
    "        'ict_index',\n",
    "        'fixture',\n",
    "        'was_home'      # we will use 'fixture' and 'was_home' to retrieve fixture difficulty\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we write code to extract fixture difficulty based on the columns 'fixture' and 'was_home'\n",
    "fixtures_22_23 = pd.read_csv('../data/raw/2022-23/fixtures.csv')\n",
    "fixtures_dict_22_23 = fixtures_22_23[['id', 'team_h_difficulty', 'team_a_difficulty']].set_index('id').T.to_dict()\n",
    "\n",
    "def add_fixture_difficulty(row, fixtures_dict):\n",
    "    fixture_id = row['fixture']\n",
    "    if row['was_home']:\n",
    "        team = 'team_h_difficulty'\n",
    "    else:\n",
    "        team = 'team_a_difficulty'\n",
    "    row['fixture_difficulty'] = fixtures_dict[fixture_id][team]\n",
    "\n",
    "    return row\n",
    "\n",
    "merged_gws_22_23 = merged_gws_22_23.apply(lambda row: add_fixture_difficulty(row=row, fixtures_dict=fixtures_dict_22_23), axis=1)\n",
    "\n",
    "# save processed data\n",
    "merged_gws_22_23.to_csv('../data/processed/2022-23/processed_merged_gws.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2023-24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in raw data\n",
    "merged_gws_23_24 = pd.read_csv('../data/raw/2023-24/gws/merged_gw.csv')\n",
    "\n",
    "# pick out useful subset of data\n",
    "merged_gws_23_24 = merged_gws_23_24[\n",
    "    [\n",
    "        'element',      # player ID\n",
    "        'name',\n",
    "        'position',\n",
    "        'GW',\n",
    "        'total_points',\n",
    "        'value',\n",
    "        'minutes',\n",
    "        'expected_goals',\n",
    "        'expected_assists',\n",
    "        'expected_goals_conceded',\n",
    "        'goals_scored',\n",
    "        'assists',\n",
    "        'goals_conceded',\n",
    "        'clean_sheets',\n",
    "        'ict_index',\n",
    "        'fixture',\n",
    "        'was_home'      # we will use 'fixture' and 'was_home' to retrieve fixture difficulty\n",
    "    ]\n",
    "]\n",
    "\n",
    "# here we write code to extract fixture difficulty based on the columns 'fixture' and 'was_home'\n",
    "fixtures_23_24 = pd.read_csv('../data/raw/2023-24/fixtures.csv')\n",
    "fixtures_dict_23_24 = fixtures_23_24[['id', 'team_h_difficulty', 'team_a_difficulty']].set_index('id').T.to_dict()\n",
    "\n",
    "merged_gws_23_24 = merged_gws_23_24.apply(lambda row: add_fixture_difficulty(row=row, fixtures_dict=fixtures_dict_23_24), axis=1)\n",
    "\n",
    "# save processed data\n",
    "merged_gws_23_24.to_csv('../data/processed/2023-24/processed_merged_gws.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2024-25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in raw data\n",
    "merged_gws_24_25 = pd.read_csv('../data/raw/2024-25/gws/merged_gw.csv')\n",
    "\n",
    "# pick out useful subset of data\n",
    "merged_gws_24_25 = merged_gws_24_25[\n",
    "    [\n",
    "        'element',      # player ID\n",
    "        'name',\n",
    "        'position',\n",
    "        'GW',\n",
    "        'total_points',\n",
    "        'value',\n",
    "        'minutes',\n",
    "        'expected_goals',\n",
    "        'expected_assists',\n",
    "        'expected_goals_conceded',\n",
    "        'goals_scored',\n",
    "        'assists',\n",
    "        'goals_conceded',\n",
    "        'clean_sheets',\n",
    "        'ict_index',\n",
    "        'fixture',\n",
    "        'was_home'      # we will use 'fixture' and 'was_home' to retrieve fixture difficulty\n",
    "    ]\n",
    "]\n",
    "\n",
    "# here we write code to extract fixture difficulty based on the columns 'fixture' and 'was_home'\n",
    "fixtures_24_25 = pd.read_csv('../data/raw/2024-25/fixtures.csv')\n",
    "fixtures_dict_24_25 = fixtures_24_25[['id', 'team_h_difficulty', 'team_a_difficulty']].set_index('id').T.to_dict()\n",
    "\n",
    "merged_gws_24_25 = merged_gws_24_25.apply(lambda row: add_fixture_difficulty(row=row, fixtures_dict=fixtures_dict_24_25), axis=1)\n",
    "\n",
    "# save processed data\n",
    "merged_gws_24_25.to_csv('../data/processed/2024-25/processed_merged_gws.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving On: Model Development\n",
    "\n",
    "Now that we have extracted the useful parts of the raw data, we can move on with the process of developing our model.\n",
    "\n",
    "**Model Selection**: For this project, we will set out to develop two models, one using a simple linear regression and one using XGBoost, and compare the performances of the two models. \n",
    "\n",
    "We will proceed in the following order:\n",
    "- **Continued Feature Engineering**: In addition to single gameweek data, we might want to create time-based cumulative features, such as ```total points in past 3 weeks```, ```minutes played in past 3 weeks```, or ```expected goal involvements in past 3 weeks```. These features may provide greater insight regarding player form.\n",
    "\n",
    "- **Feature Selection**: We may want to perform correlation analysis to evaluate feature importance, as well as finding features that have high correlation with each other (so we can remove redundant dimensions).\n",
    "\n",
    "- **Feature Scaling and Encoding**: As we are going to use a linear model (as one of our models), we will have to perform feature scaling and encoding, which includes data normalization, one-hot encoding, and more.\n",
    "\n",
    "- **Splitting Data for Training and Evaluation**: A key part to devleoping any type of machine learning model. We will split our data into training & testing (and depending on performance needs, validation) sets.\n",
    "\n",
    "- **Model Training & Evaluation**: Finally, we will train our models and evaluate them using metrics such as MSE. Depending on how good our models are, we may want to revise some of the previous steps mentioned and make improvements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xfpl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
